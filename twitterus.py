# -*- coding: utf-8 -*-
"""TwitterUS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vfm402-rkJZg3Er51lx0Dw9fIrnOzbR7
"""

import pandas as pd
x_test=pd.read_csv("test_twitter_x_test.csv")
x_train=pd.read_csv("training_twitter_x_y_train.csv")

x_train.shape

x_test.shape

x_train.head(5)

x_test.head(5)

x_train.columns

x_train_text=x_train["text"]
x_train_senti=x_train["airline_sentiment"]

x_test_text=x_test["text"]

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
x_train_tokenize_text=[]
for x in x_train_text:
  x_train_tokenize_text.append(word_tokenize(x))
print(len(x_train_tokenize_text))

from nltk.tokenize import word_tokenize
x_test_tokenize_text=[]
for x in x_test_text:
  x_test_tokenize_text.append(word_tokenize(x))
print(len(x_test_tokenize_text))

train_documents=[]
for i in range(len(x_train)):
  train_documents.append((x_train_tokenize_text[i],x_train_senti[i]))
print(train_documents[0:5])

import random
random.shuffle(train_documents)
train_documents[0:5]

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

import nltk
nltk.download('wordnet')

from nltk.corpus import wordnet
def get_simple_pos(tag):
  if str(tag).startswith('J'):
    return wordnet.ADJ
  elif str(tag).startswith('V'):
    return wordnet.VERB
  elif str(tag).startswith('N'):
    return wordnet.NOUN
  elif str(tag).startswith('R'):
    return wordnet.ADV
  else:
    return wordnet.NOUN

import nltk
nltk.download('averaged_perceptron_tagger')

nltk.download('stopwords')
from nltk.corpus import stopwords
stop=stopwords.words("english")
import string
punctuations=list(string.punctuation)
stop=stop+punctuations

stop

from nltk import pos_tag
from nltk.tokenize import sent_tokenize,word_tokenize
w="good"
pos_tag(word_tokenize(w))

def clean_review(words):
  output_words=[]
  for w in words:
    if w.lower() not in stop:
      print(w)
      pos=pos_tag([w])
      clean_word=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[[0][0]]))
      output_words.append(clean_word.lower())
  return output_words

train_documents=[(clean_review(document),category) for document,category in train_documents]

train_documents[0]

all_words=[]
for doc in train_documents:
  all_words+=doc[0]
freq=nltk.FreqDist(all_words)
common=freq.most_common(1000)
common

features=[i[0] for i in common]

def get_feature_dict(words):
  current_features={}
  words_set=set(words)
  for w in features:
    current_features[w]=w in words_set
  return current_features

get_feature_dict(train_documents[0][0])

training_data=[(get_feature_dict(doc),category) for doc,category in train_documents]

from nltk import NaiveBayesClassifier
classifier=NaiveBayesClassifier.train(training_data)

x_test_tokenize_text

test_documents=[clean_review(document) for document in x_test_tokenize_text]

get_feature_dict(test_documents[0][0])

testing_data=[get_feature_dict(doc) for doc in test_documents]

testing_data

len(testing_data)

testing_data[0]

result=[]
for x in testing_data:
  result.append(classifier.classify(x))
result

# Import the csv module
import csv

# Specify the file path
file_path = 'predictions.csv'  # You can change this to your desired file path

# Open the CSV file in write mode
with open(file_path, 'w', newline='') as file:
    # Create a CSV writer object
    writer = csv.writer(file)

    # Write each element of the list as a separate column
    for element in result:
        writer.writerow([element])